{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eecf4cd-3d55-4a5a-8b93-d086cb08dd96",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Loading the libraries\n",
    "\n",
    "---\n",
    "\n",
    "Load các library cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0278266f-5506-4bd4-a667-4b4c6e1f5ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import weaviate\n",
    "from weaviate.classes.query import (\n",
    "    Filter, \n",
    "    Rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a279b83-4419-4b42-a397-cb4c12111436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'flask_app'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "import flask_app\n",
    "import weaviate_server\n",
    "from utils import (\n",
    "    generate_with_single_input,\n",
    "    print_object_properties,\n",
    "    display_widget\n",
    ")\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f03d249-d1e6-479c-bc52-d12d7dd9fc94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4755d3c-8f13-4358-9bac-8970e226c865",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Setting up the Weaviate Client and loading the data\n",
    "\n",
    "---\n",
    "\n",
    "Thiết lập Weaviate client và load data từ bộ dữ liệu [BBC news dataset](https://www.kaggle.com/datasets/gpreda/bbc-news) adapted từ Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfd89f-b03f-49eb-84d5-81615af34601",
   "metadata": {},
   "source": [
    "<a id='2-1'></a>\n",
    "### 2.1 Loading the Weaviate Client\n",
    "\n",
    "Let's connect the Weaviate client to begin working with the Weaviate API. The server is already running on the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d11b182-b1e6-4dc8-8cb9-7e7aec4743fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_local(port=8079, grpc_port=50050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b2f2d-058f-4d1a-a4a5-f0ea444c6bb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Loading the data\n",
    "\n",
    "Load data. Bộ dữ liệu được cấu trúc với các fields sau:\n",
    "\n",
    "- **`title`**: Tiêu đề của bài báo.\n",
    "- **`pubDate`**: Ngày và giờ xuất bản của bài báo.\n",
    "- **`guid`**: Mã định danh duy nhất (unique identifier) cho bài báo, thường được dùng để listing.\n",
    "- **`link`**: Đường dẫn URL để truy cập bài báo đầy đủ trực tuyến.\n",
    "- **`description`**: Một đoạn tóm tắt ngắn hoặc nội dung giới thiệu (teaser) của bài báo.\n",
    "- **`article_content`**: Toàn bộ nội dung văn bản của bài báo, cung cấp thông tin chi tiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d4a42-6962-4584-b416-442ce21f7ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbc_data = joblib.load('data/bbc_data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6d6a5-4c48-4da5-b10e-121fea5f548d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_object_properties(bbc_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44b5f4-f2c2-4f3d-a0dc-59c00f6d91c0",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Loading the Collection\n",
    "\n",
    "---\n",
    "\n",
    "Load bộ collection có chứa BBC News dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a40b5f-2e40-4e05-9642-dbcb2e44bb01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection = client.collections.get(\"bbc_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d074db-a6ad-4a81-a9ed-776e32c084a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"The number of elements in the collection is: {len(collection)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd747ff-6c62-44bb-a6f8-62ae5a66fdc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object = collection.query.fetch_objects(limit = 1, include_vector = True).objects[0]\n",
    "print(\"Printing the properties (some will be truncated due to size)\")\n",
    "print_object_properties(object.properties)\n",
    "print(\"Vector: (truncated)\",object.vector['main_vector'][0:15])\n",
    "print(\"Vector length: \", len(object.vector['main_vector']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1562a0d-cbc9-4425-9365-28eeac062c1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='ex01'></a>\n",
    "\n",
    "<a id='3-1'></a>\n",
    "### 3.1 Metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe652b1f-91c3-4726-9502-4b6ddb5f4430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_by_metadata(metadata_property: str, \n",
    "                       values: list[str], \n",
    "                       collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                       limit: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves objects từ một collection được chỉ định dựa trên các tiêu chí metadata filtering.\n",
    "\n",
    "    Hàm này truy vấn một collection trong client được chỉ định để lấy các objects khớp với các tiêu chí metadata nhất định. Nó sử dụng một filter để tìm các objects có 'property' được chỉ định chứa bất kỳ giá trị nào trong số các 'values' đã cho. Số lượng objects được truy xuất bị giới hạn bởi tham số 'limit'.\n",
    "\n",
    "    Args:\n",
    "    metadata_property (str): Tên của metadata property dùng để lọc.\n",
    "    values (List[str]): Một danh sách các giá trị để đối chiếu với property đã chỉ định.\n",
    "    collection_name (weaviate.collections.collection.sync.Collection): Collection để thực hiện truy vấn.\n",
    "    limit (int, optional): Số lượng objects tối đa được truy xuất. Mặc định là 5.\n",
    "\n",
    "    Returns:\n",
    "    List[Object]: Một danh sách các objects từ collection khớp với các tiêu chí lọc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve using collection.query.fetch_objects\n",
    "    \n",
    "    response = collection.query.fetch_objects(\n",
    "        filters = Filter.by_property(metadata_property).contains_any(values),\n",
    "        limit=limit)\n",
    "\n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    \n",
    "    return response_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84067ad0-9f79-4ba8-a64c-4fa669fb88a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "res = filter_by_metadata('title', ['Taylor Swift'], collection, limit = 2)\n",
    "for x in res:\n",
    "    print_object_properties(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf4782-5927-472d-bae9-57f3793b6605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your solution!\n",
    "unittests.test_filter_by_metadata(filter_by_metadata, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a89bf-5d06-4566-bef5-473c12ae3e06",
   "metadata": {},
   "source": [
    "<a id='ex02'></a>\n",
    "\n",
    "<a id='3-2'></a>\n",
    "### 3.2 Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ed7fd-afb4-4dff-9805-28642dfa4d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def semantic_search_retrieve(query: str,\n",
    "                             collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                             top_k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Thực hiện một semantic search trên một collection và truy xuất các chunks liên quan nhất.\n",
    "\n",
    "    Hàm này thực thi một truy vấn semantic search trên một collection được chỉ định để tìm các text chunks có độ liên quan cao nhất với đầu vào 'query'. Phép tra cứu truy xuất một số lượng giới hạn các matching objects hàng đầu, được chỉ định bởi 'top_k'. Hàm trả về thuộc tính 'chunk' của mỗi top matching objects đó.\n",
    "\n",
    "    Args:\n",
    "    query (str): Truy vấn tìm kiếm được sử dụng để tìm các text chunks liên quan.\n",
    "    collection (weaviate.collections.collection.sync.Collection): Collection nơi phép semantic search được thực hiện.\n",
    "    top_k (int, optional): Số lượng các relevant objects hàng đầu cần truy xuất. Mặc định là 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Một danh sách các text chunks có độ liên quan cao nhất với truy vấn đã cho.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve using collection.query.near_text\n",
    "    response = collection.query.near_text(query=query, limit=top_k)\n",
    "    \n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    \n",
    "    return response_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee6728-5def-429c-840f-94f19c2e1b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's have an example!\n",
    "print_object_properties(semantic_search_retrieve(query = 'Tell me about the last Taylor Swift show', collection = collection, top_k = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2605c-6784-468f-8ab8-7aed31f0af20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unittests.test_semantic_search_retrieve(semantic_search_retrieve, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9c3ee-681f-4e94-bad0-2f4dba5e3081",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='ex03'></a>\n",
    "\n",
    "<a id='3-3'></a>\n",
    "### 3.3 BM25 Serach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64579271-9ba8-4588-bf50-a2f2a29eebc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bm25_retrieve(query: str, \n",
    "                  collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                  top_k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Thực hiện một BM25 search trên một collection và truy xuất các chunks liên quan nhất.\n",
    "    Hàm này thực thi một truy vấn tìm kiếm dựa trên BM25 trên một collection được chỉ định để xác định các text chunks có độ liên quan cao nhất với 'query' được cung cấp. Nó truy xuất một số lượng giới hạn các matching objects hàng đầu, được chỉ định bởi 'top_k', và trả về thuộc tính 'chunk' của các objects này.\n",
    "\n",
    "    Args:\n",
    "    query (str): Truy vấn tìm kiếm được sử dụng để tìm các text chunks liên quan.\n",
    "    collection (weaviate.collections.collection.sync.Collection): Collection nơi phép BM25 search được thực hiện.\n",
    "    top_k (int, optional): Số lượng các relevant objects hàng đầu cần truy xuất. Mặc định là 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Một danh sách các text chunks có độ liên quan cao nhất với truy vấn đã cho.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve using collection.query.bm25\n",
    "    response = collection.query.bm25(\n",
    "        query=query,\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    return response_objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab4432-eb56-4aee-8ca6-bd4157baf14a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_object_properties(bm25_retrieve('Tell me about the last Taylor Swift show', collection, top_k = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d370a83-47c2-4aeb-a0eb-d4c755f37abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unittests.test_bm25_retrieve(bm25_retrieve, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad576d-5c32-456b-b2ac-6c27bb6db82b",
   "metadata": {},
   "source": [
    "<a id='ex04'></a>\n",
    "\n",
    "<a id='3-4'></a>\n",
    "### 3.4 Hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657f058-4f71-4eca-8ecb-28eabaf45170",
   "metadata": {},
   "source": [
    "Triển khai một hệ thống retrieval sử dụng Reciprocal Rank Fusion (RRF) thông qua Weaviate API. Để đạt được điều này, cần sử dụng phương thức `collection.query.hybrid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698b773-0e88-41ad-b6e6-fac503e488db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hybrid_retrieve(query: str, \n",
    "                    collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                    alpha: float = 0.5,\n",
    "                    top_k: int = 5\n",
    "                   ) -> list:\n",
    "    \"\"\"\n",
    "    Thực hiện một hybrid search trên một collection và truy xuất các chunks liên quan nhất.\n",
    "    Hàm này thực thi một truy vấn hybrid search kết hợp giữa semantic vector search và tìm kiếm dựa trên từ khóa truyền thống (keyword-based search) trên một collection được chỉ định để tìm các text chunks có độ liên quan cao nhất với đầu vào 'query'. Độ liên quan của kết quả bị ảnh hưởng bởi tham số 'alpha', giúp cân bằng trọng số giữa khớp vector (vector matches) và khớp từ khóa (keyword matches). Nó truy xuất một số lượng giới hạn các matching objects hàng đầu, được chỉ định bởi 'top_k', và trả về thuộc tính 'chunk' của các objects này.\n",
    "\n",
    "    Args:\n",
    "    query (str): Truy vấn tìm kiếm được sử dụng để tìm các text chunks liên quan.\n",
    "    collection (weaviate.collections.collection.sync.Collection): Collection nơi phép hybrid search được thực hiện.\n",
    "    alpha (float, optional): Một hệ số trọng số giúp cân bằng sự đóng góp của semantic matches và keyword matches. Mặc định là 0.5.\n",
    "    top_k (int, optional): Số lượng các relevant objects hàng đầu cần truy xuất. Mặc định là 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Một danh sách các text chunks có độ liên quan cao nhất với truy vấn đã cho.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve using collection.query.hybrid\n",
    "    response = collection.query.hybrid(\n",
    "        query=query,\n",
    "        alpha=alpha,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    \n",
    "    return response_objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfad86-c65f-44fb-a99f-83fc7971cb57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_object_properties(hybrid_retrieve('Tell me about the last Taylor Swift show', collection, top_k = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71476b02-6134-4b03-b359-dc31471a1af2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.5 Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b2ed3-5a09-4f27-875e-fe273c62adcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_query_type(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Phân loại query thành 'technical' hoặc 'creative'. \n",
    "    \n",
    "    - Technical: Câu hỏi về sự kiện, số liệu, thông tin cụ thể\n",
    "    - Creative: Yêu cầu tổng hợp, phân tích, đưa ra ý kiến\n",
    "    \n",
    "    Args:\n",
    "        query (str): Câu hỏi của người dùng\n",
    "        \n",
    "    Returns: \n",
    "        str: 'technical' hoặc 'creative'\n",
    "    \"\"\"\n",
    "    PROMPT = f\"\"\"Analyze the following query and classify it as either 'technical' or 'creative'. \n",
    "\n",
    "Technical queries: \n",
    "- Ask for specific facts, dates, numbers, or events\n",
    "- Request documentation or procedural information\n",
    "- Seek objective, factual answers\n",
    "\n",
    "Creative queries: \n",
    "- Ask for analysis, opinions, or interpretations\n",
    "- Request summaries or comparisons\n",
    "- Seek subjective or exploratory answers\n",
    "\n",
    "Query:  {query}\n",
    "\n",
    "Answer only 'technical' or 'creative' (one word, lowercase).\n",
    "\"\"\"\n",
    "    result = generate_with_single_input(PROMPT, max_tokens=5)\n",
    "    label = result['content'].strip().lower()\n",
    "    \n",
    "    # Validate output\n",
    "    if label not in ['technical', 'creative']:\n",
    "        label = 'technical'  # Default fallback\n",
    "    \n",
    "    return label\n",
    "\n",
    "\n",
    "def classify_news_category(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Phân loại query theo danh mục tin tức để tối ưu retrieval.\n",
    "    \n",
    "    Categories:  politics, sports, entertainment, business, technology, general\n",
    "    \n",
    "    Args:\n",
    "        query (str): Câu hỏi của người dùng\n",
    "        \n",
    "    Returns: \n",
    "        str:  Danh mục tin tức\n",
    "    \"\"\"\n",
    "    PROMPT = f\"\"\"Classify the following news query into one of these categories:\n",
    "- politics:  Government, elections, international relations\n",
    "- sports:  Games, athletes, tournaments\n",
    "- entertainment: Movies, music, celebrities\n",
    "- business: Economy, markets, companies\n",
    "- technology: Tech companies, innovations, digital\n",
    "- general: Other topics\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Answer with only ONE category name (lowercase).\n",
    "\"\"\"\n",
    "    result = generate_with_single_input(PROMPT, max_tokens=5)\n",
    "    category = result['content'].strip().lower()\n",
    "    \n",
    "    valid_categories = ['politics', 'sports', 'entertainment', 'business', 'technology', 'general']\n",
    "    if category not in valid_categories:\n",
    "        category = 'general'\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a231cb7-d5ff-4700-bfca-566cf72d78bc",
   "metadata": {},
   "source": [
    "### 3.6 Dynamic Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98795b-f2fc-44c7-9804-2a26f33e1cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm_params_for_query(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Tự động chọn tham số LLM dựa trên loại query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Câu hỏi của người dùng\n",
    "        \n",
    "    Returns: \n",
    "        dict:  Tham số cho LLM call (temperature, top_p)\n",
    "    \"\"\"\n",
    "    query_type = classify_query_type(query)\n",
    "    \n",
    "    if query_type == 'technical': \n",
    "        # Technical:  Cần chính xác, ít ngẫu nhiên\n",
    "        params = {\n",
    "            'temperature':  0.1,\n",
    "            'top_p': 0.1,\n",
    "            'description': 'Technical mode:  Low randomness for factual accuracy'\n",
    "        }\n",
    "    else:  # creative\n",
    "        # Creative: Cho phép sáng tạo hơn\n",
    "        params = {\n",
    "            'temperature':  0.7,\n",
    "            'top_p': 0.4,\n",
    "            'description': 'Creative mode: Higher randomness for diverse responses'\n",
    "        }\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def get_retrieval_params_for_query(query:  str) -> dict:\n",
    "    \"\"\"\n",
    "    Tự động chọn tham số retrieval dựa trên loại query.\n",
    "    \"\"\"\n",
    "    query_type = classify_query_type(query)\n",
    "    category = classify_news_category(query)\n",
    "    \n",
    "    # Base params - mặc định KHÔNG dùng rerank để tránh lỗi\n",
    "    params = {\n",
    "        'top_k': 5,\n",
    "        'alpha': 0.5,\n",
    "        'use_rerank': False,  # Default: không rerank\n",
    "        'rerank_property': 'chunk'\n",
    "    }\n",
    "    \n",
    "    if query_type == 'technical':\n",
    "        params['top_k'] = 7\n",
    "        params['alpha'] = 0.3\n",
    "        # Chỉ bật rerank cho technical queries quan trọng\n",
    "        # Có thể bật lại nếu muốn:  params['use_rerank'] = True\n",
    "    else:  \n",
    "        params['top_k'] = 5\n",
    "        params['alpha'] = 0.7\n",
    "    \n",
    "    if category in ['politics', 'business']: \n",
    "        params['top_k'] += 2\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c152653-e2a0-4661-8ee1-f61476ecce19",
   "metadata": {},
   "source": [
    "### 3.7 Structured Output Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b0c47-b76f-4c88-ba76-ceeff1d0ad71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NewsSource(BaseModel):\n",
    "    \"\"\"Schema cho một nguồn tin tức\"\"\"\n",
    "    title: str = Field(description=\"Tiêu đề bài báo\")\n",
    "    url: str = Field(description=\"Link đến bài báo\")\n",
    "    published_date: str = Field(description=\"Ngày xuất bản\")\n",
    "    relevance:  str = Field(description=\"Mức độ liên quan:  high/medium/low\")\n",
    "\n",
    "\n",
    "class RAGResponse(BaseModel):\n",
    "    \"\"\"Schema cho response của RAG system\"\"\"\n",
    "    answer: str = Field(description=\"Câu trả lời chi tiết cho query\")\n",
    "    summary: str = Field(description=\"Tóm tắt 1-2 câu\")\n",
    "    confidence: str = Field(description=\"Độ tin cậy: high/medium/low\")\n",
    "    sources: List[NewsSource] = Field(description=\"Danh sách các nguồn được sử dụng\")\n",
    "    query_type: str = Field(description=\"Loại query:  technical/creative\")\n",
    "    category: str = Field(description=\"Danh mục tin tức\")\n",
    "\n",
    "\n",
    "class FactCheckResponse(BaseModel):\n",
    "    \"\"\"Schema cho việc kiểm tra thông tin\"\"\"\n",
    "    claim: str = Field(description=\"Thông tin cần kiểm tra\")\n",
    "    verdict:  Literal[\"true\", \"false\", \"partially_true\", \"unverifiable\"] = Field(\n",
    "        description=\"Kết luận\"\n",
    "    )\n",
    "    evidence: List[str] = Field(description=\"Bằng chứng từ các nguồn\")\n",
    "    sources: List[NewsSource] = Field(description=\"Nguồn tham khảo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff49c08-572f-4045-a041-29462e70904f",
   "metadata": {},
   "source": [
    "### 3.8 - Reranking\n",
    "\n",
    "Tạo một phiên bản mới của `semantic_search` cho phép thực hiện reranking các kết quả. Hàm mới này hỗ trợ việc sử dụng một query khác cho mục đích reranking hoặc thực hiện reranking dựa trên một thuộc tính tài liệu cụ thể (ví dụ: reranking chỉ sử dụng trường title).\n",
    "\n",
    "Nhiệm vụ là phải thêm tham số `rerank` vào lệnh gọi `collection.query.near_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c331636-7382-4d94-b24d-8ac9feaf0871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def semantic_search_with_reranking(query: str, \n",
    "                                   rerank_property: str,\n",
    "                                   collection: \"weaviate.collections.collection.sync.Collection\" , \n",
    "                                   rerank_query: str = None,\n",
    "                                   top_k: int = 5\n",
    "                                   ) -> list:\n",
    "    \"\"\"\n",
    "    Thực hiện một semantic search và thực hiện reranks các kết quả dựa trên một property được chỉ định.\n",
    "\n",
    "    Args:\n",
    "    query (str): Truy vấn tìm kiếm để thực hiện bước tìm kiếm ban đầu (initial search).\n",
    "    rerank_property (str): Thuộc tính được sử dụng để reranking các kết quả tìm kiếm.\n",
    "    collection (weaviate.collections.collection.sync.Collection): Collection để thực hiện tìm kiếm bên trong.\n",
    "    rerank_query (str, optional): Truy vấn được sử dụng riêng cho mục đích reranking. Nếu không được cung cấp, truy vấn gốc (original query) sẽ được sử dụng để reranking.\n",
    "    top_k (int, optional): Số lượng kết quả hàng đầu tối đa được trả về. Mặc định là 5.\n",
    "\n",
    "    Returns:\n",
    "    list: Một danh sách các properties từ các kết quả tìm kiếm đã được reranked, trong đó mỗi mục tương ứng với một object trong collection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the rerank_query to be the same as the query if rerank_query is not passed\n",
    "    if rerank_query is None: \n",
    "        rerank_query = query \n",
    "        \n",
    "    # Define the reranker with rerank_query and rerank_property\n",
    "    reranker = Rerank(\n",
    "        prop=rerank_property,\n",
    "        query=rerank_query\n",
    "    )\n",
    "\n",
    "    # Retrieve using collection.query.near_text with the appropriate parameters (do not forget the rerank!)\n",
    "    response = collection.query.near_text(\n",
    "        query=query,\n",
    "        limit=top_k,\n",
    "        rerank=reranker\n",
    "    )\n",
    "    \n",
    "    response_objects = [x.properties for x in response.objects]\n",
    "    \n",
    "    return response_objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a9236-5cb6-4ab0-8e41-4695c8179371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a query\n",
    "query = 'Tell me about the conflicts in Latin America'\n",
    "# Get the results from a search (in this case the hybrid search)\n",
    "results = semantic_search_with_reranking(query, collection = collection, top_k = 2, rerank_property = 'chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6f020-c73a-4333-93bd-273c9677ddb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_object_properties(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75a341-4c1e-4b67-b6bb-4908b281c826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your function!\n",
    "unittests.test_semantic_search_with_reranking(semantic_search_with_reranking, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13571fc-eb69-4248-9d6a-f9b73da36c4a",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Incorporating the Weaviate API into our previous schema\n",
    "---\n",
    "\n",
    "Tại đây, bạn sẽ xem lại các hàm đã được sử dụng xuyên suốt để tích hợp Weaviate API vào schema hiện tại của mình. Sau khi tích hợp, bạn sẽ có thể chạy các prompts và kiểm tra hệ thống RAG system mới của mình!\n",
    "\n",
    "<a id='4-1'></a>\n",
    "### 4.1 Generating the final prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d03a0-fc4e-4caa-8862-024c40ab9184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_final_prompt(\n",
    "    query: str, \n",
    "    top_k: int, \n",
    "    retrieve_function:  callable,\n",
    "    rerank_query: str = None, \n",
    "    rerank_property: str = None, \n",
    "    use_rerank: bool = False, \n",
    "    use_rag: bool = True,\n",
    "    output_format: str = \"text\",\n",
    "    include_classification: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates a final prompt với các tính năng prompt engineering. \n",
    "    \"\"\"\n",
    "    # Nếu không dùng RAG, trả về query gốc\n",
    "    if not use_rag:\n",
    "        return query\n",
    "    \n",
    "    # Phân loại query\n",
    "    query_type = classify_query_type(query) if include_classification else \"general\"\n",
    "    category = classify_news_category(query) if include_classification else \"general\"\n",
    "    \n",
    "    # Retrieve documents\n",
    "    if use_rerank:\n",
    "        if rerank_property is None:\n",
    "            rerank_property = 'chunk'  # Default property\n",
    "        # Khi cần rerank, PHẢI dùng semantic_search_with_reranking\n",
    "        top_k_documents = semantic_search_with_reranking(\n",
    "            query=query, \n",
    "            top_k=top_k, \n",
    "            collection=collection, \n",
    "            rerank_property=rerank_property, \n",
    "            rerank_query=rerank_query\n",
    "        )\n",
    "    else:\n",
    "        # Dùng retrieve_function được truyền vào (hybrid, semantic, bm25)\n",
    "        top_k_documents = retrieve_function(\n",
    "            query=query, \n",
    "            top_k=top_k, \n",
    "            collection=collection\n",
    "        )\n",
    "    \n",
    "    # Format documents\n",
    "    formatted_data = \"\"\n",
    "    source_list = []\n",
    "    \n",
    "    for i, document in enumerate(top_k_documents, 1):\n",
    "        document_layout = (\n",
    "            f\"[Source {i}]\\n\"\n",
    "            f\"Title: {document['title']}\\n\"\n",
    "            f\"Content: {document['chunk']}\\n\"\n",
    "            f\"Published: {document['pubDate']}\\n\"\n",
    "            f\"URL:  {document['link']}\\n\"\n",
    "        )\n",
    "        formatted_data += document_layout + \"\\n\"\n",
    "        \n",
    "        source_list.append({\n",
    "            \"title\":  document['title'],\n",
    "            \"url\": document['link'],\n",
    "            \"published_date\": str(document['pubDate'])\n",
    "        })\n",
    "    \n",
    "    # Xây dựng prompt dựa trên output format\n",
    "    if output_format == \"json\":\n",
    "        prompt = _build_json_prompt(query, query_type, category, formatted_data, source_list)\n",
    "    else:\n",
    "        prompt = _build_text_prompt(query, query_type, category, formatted_data)\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _build_text_prompt(query: str, query_type: str, category: str, formatted_data: str) -> str:\n",
    "    \"\"\"Xây dựng prompt cho text output\"\"\"\n",
    "    \n",
    "    # Điều chỉnh instructions dựa trên query type\n",
    "    if query_type == \"technical\":\n",
    "        style_instruction = \"\"\"\n",
    "- Be precise and factual\n",
    "- Include specific dates, numbers, and names when available\n",
    "- Cite sources explicitly\n",
    "- Avoid speculation\n",
    "\"\"\"\n",
    "    else:  # creative\n",
    "        style_instruction = \"\"\"\n",
    "- Provide analysis and context\n",
    "- Connect different pieces of information\n",
    "- Offer insights and implications\n",
    "- Use engaging language\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a news analyst assistant. Answer the user's query using the provided news sources.\n",
    "\n",
    "**Query Type**:  {query_type}\n",
    "**Category**: {category}\n",
    "\n",
    "**Instructions**:\n",
    "{style_instruction}\n",
    "- Always cite your sources using [Source N] format\n",
    "- If information is uncertain, acknowledge it\n",
    "- Structure your response clearly\n",
    "\n",
    "**User Query**: {query}\n",
    "\n",
    "**Available News Sources**:\n",
    "{formatted_data}\n",
    "\n",
    "**Your Response**:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _build_json_prompt(\n",
    "    query: str, \n",
    "    query_type: str, \n",
    "    category: str, \n",
    "    formatted_data: str, \n",
    "    source_list: list\n",
    ") -> str:\n",
    "    \"\"\"Xây dựng prompt cho JSON output\"\"\"\n",
    "    \n",
    "    json_schema = RAGResponse. model_json_schema()\n",
    "    \n",
    "    prompt = f\"\"\"You are a news analyst assistant. Answer the user's query and return a structured JSON response.\n",
    "\n",
    "**Query Type**: {query_type}\n",
    "**Category**: {category}\n",
    "\n",
    "**User Query**: {query}\n",
    "\n",
    "**Available News Sources**:\n",
    "{formatted_data}\n",
    "\n",
    "**Instructions**:\n",
    "- Respond ONLY with valid JSON matching the schema below\n",
    "- Fill in all required fields\n",
    "- Be accurate and cite sources properly\n",
    "\n",
    "**JSON Schema**:\n",
    "{json. dumps(json_schema, indent=2)}\n",
    "\n",
    "**Your JSON Response**:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d08665-55b7-462b-9067-22d32ee6fbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = generate_final_prompt(\"Tell me the economic situation of the US in 2024.\", top_k = 5, retrieve_function = semantic_search_retrieve, use_rerank = False, rerank_property = 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e269a85-1686-42f3-8435-c452fe2dbad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f02ed6-1f16-4c0f-84d2-86384fd5f2ae",
   "metadata": {},
   "source": [
    "<a id='4-2'></a>\n",
    "### 4.2 LLM call\n",
    "\n",
    "Hãy xem lại hàm llm_call, hiện đã được adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4b9c0-d718-4a37-b8d1-9f9157ab08d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_call(\n",
    "    query: str, \n",
    "    retrieve_function: callable = None, \n",
    "    top_k: int = None,\n",
    "    use_rag: bool = True, \n",
    "    use_rerank: bool = None,\n",
    "    rerank_property: str = None, \n",
    "    rerank_query: str = None,\n",
    "    output_format: str = \"text\",\n",
    "    auto_tune: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Enhanced LLM call với prompt engineering features.\n",
    "    \"\"\"\n",
    "    # Default retrieve function\n",
    "    if retrieve_function is None:  \n",
    "        retrieve_function = hybrid_retrieve\n",
    "    \n",
    "    # Auto-tune parameters nếu được bật\n",
    "    if auto_tune:\n",
    "        llm_params = get_llm_params_for_query(query)\n",
    "        retrieval_params = get_retrieval_params_for_query(query)\n",
    "        \n",
    "        if top_k is None:  \n",
    "            top_k = retrieval_params['top_k']\n",
    "        if use_rerank is None:  \n",
    "            use_rerank = retrieval_params['use_rerank']\n",
    "        if rerank_property is None and use_rerank: \n",
    "            rerank_property = retrieval_params. get('rerank_property', 'chunk')\n",
    "    else:\n",
    "        llm_params = {'temperature': 0.5, 'top_p': 0.5}\n",
    "        if top_k is None: \n",
    "            top_k = 5\n",
    "        if use_rerank is None:  \n",
    "            use_rerank = False\n",
    "    \n",
    "    # Lấy thông tin classification\n",
    "    query_type = classify_query_type(query)\n",
    "    category = classify_news_category(query)\n",
    "    \n",
    "    # Generate prompt\n",
    "    # LƯU Ý: Khi use_rerank=True, generate_final_prompt sẽ tự động\n",
    "    # sử dụng semantic_search_with_reranking thay vì retrieve_function\n",
    "    PROMPT = generate_final_prompt(\n",
    "        query=query, \n",
    "        top_k=top_k, \n",
    "        retrieve_function=retrieve_function,  # Chỉ dùng khi use_rerank=False\n",
    "        use_rag=use_rag, \n",
    "        use_rerank=use_rerank, \n",
    "        rerank_property=rerank_property, \n",
    "        rerank_query=rerank_query,\n",
    "        output_format=output_format,\n",
    "        include_classification=True\n",
    "    )\n",
    "    \n",
    "    # Gọi LLM với params đã điều chỉnh\n",
    "    if output_format == \"json\": \n",
    "        response = generate_with_multiple_input(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful news analyst.  Respond only in valid JSON. \"},\n",
    "                {\"role\": \"user\", \"content\":  PROMPT}\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\":  \"json_schema\",\n",
    "                \"schema\": RAGResponse.model_json_schema()\n",
    "            }\n",
    "        )\n",
    "        content = response['content']\n",
    "        \n",
    "        try:  \n",
    "            parsed_content = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_content = {\"error\": \"Failed to parse JSON\", \"raw\":  content}\n",
    "    else:\n",
    "        # Text output - kiểm tra xem generate_with_single_input có hỗ trợ params không\n",
    "        try:\n",
    "            response = generate_with_single_input(\n",
    "                PROMPT,\n",
    "                temperature=llm_params. get('temperature', 0.5),\n",
    "                top_p=llm_params. get('top_p', 0.5)\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Fallback nếu function không hỗ trợ temperature/top_p\n",
    "            response = generate_with_single_input(PROMPT)\n",
    "        \n",
    "        content = response['content']\n",
    "        parsed_content = None\n",
    "    \n",
    "    return {\n",
    "        'content': content,\n",
    "        'parsed_json': parsed_content,\n",
    "        'query_type': query_type,\n",
    "        'category': category,\n",
    "        'params_used': {\n",
    "            'llm':  llm_params,\n",
    "            'retrieval': {\n",
    "                'top_k': top_k,\n",
    "                'use_rerank': use_rerank,\n",
    "                'rerank_property': rerank_property\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd948b07-832c-489f-b01d-f8fd2f5bfc4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Tell me about United States and Brazil's relationship over the course of 2024. Provide links for the resources you use in the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153412e-0f8d-4517-a1d0-f37a583966bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Result with reranked results\n",
    "print(llm_call(query = query, \n",
    "               top_k = 5, \n",
    "               retrieve_function = hybrid_retrieve, \n",
    "               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c444d9-5511-45d4-a566-872a87ad3b10",
   "metadata": {},
   "source": [
    "### 4.2.1 WRAPPER Cho DISPLAY_WIDGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4498a9d-c156-4c60-9f99-a6f7e656e950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bước 1: Lưu phiên bản advanced với tên khác\n",
    "llm_call_advanced = llm_call\n",
    "\n",
    "# Bước 2: Ghi đè llm_call để trả về string (tương thích widget)\n",
    "def llm_call(\n",
    "    query: str, \n",
    "    retrieve_function:  callable = None, \n",
    "    top_k: int = 5, \n",
    "    use_rag: bool = True, \n",
    "    use_rerank: bool = False, \n",
    "    rerank_property: str = None, \n",
    "    rerank_query:  str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Phiên bản llm_call tương thích với display_widget.\n",
    "    Tự động áp dụng prompt engineering và trả về string. \n",
    "    \n",
    "    Để sử dụng phiên bản đầy đủ với dict output, dùng:  llm_call_advanced()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = llm_call_advanced(\n",
    "            query=query,\n",
    "            retrieve_function=retrieve_function,\n",
    "            top_k=top_k if top_k else 5,\n",
    "            use_rag=use_rag,\n",
    "            use_rerank=use_rerank,\n",
    "            rerank_property=rerank_property,\n",
    "            rerank_query=rerank_query,\n",
    "            output_format=\"text\",\n",
    "            auto_tune=True\n",
    "        )\n",
    "        \n",
    "        # Tạo header với metadata\n",
    "        header = f\"\"\"\n",
    "> **Query Analysis**:  `{result['query_type']}` | **Category**: `{result['category']}` | **Temp**: `{result['params_used']['llm']['temperature']}`\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "        return header + result['content']\n",
    "    \n",
    "    except Exception as e: \n",
    "        # Fallback nếu có lỗi\n",
    "        print(f\"Warning: Advanced features failed ({e}), using basic mode\")\n",
    "        \n",
    "        if retrieve_function is None: \n",
    "            retrieve_function = hybrid_retrieve\n",
    "            \n",
    "        if use_rerank and rerank_property: \n",
    "            docs = semantic_search_with_reranking(\n",
    "                query=query, \n",
    "                collection=collection, \n",
    "                top_k=top_k, \n",
    "                rerank_property=rerank_property\n",
    "            )\n",
    "        else:\n",
    "            docs = retrieve_function(query=query, collection=collection, top_k=top_k)\n",
    "        \n",
    "        formatted = \"\\n\".join([\n",
    "            f\"Title:  {d['title']}, Chunk: {d['chunk'][:200]}...\" \n",
    "            for d in docs\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"Answer based on: {query}\\nSources:\\n{formatted}\" if use_rag else query\n",
    "        response = generate_with_single_input(prompt)\n",
    "        return response['content']\n",
    "\n",
    "print(\"llm_call đã được cập nhật để tương thích với display_widget!\")\n",
    "print(\"Dùng llm_call_advanced() khi cần dict output với metadata đầy đủ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ced956-1988-4061-a349-62387187deb3",
   "metadata": {},
   "source": [
    "### 4.3 Specialized RAG Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a1372-e292-4514-b6de-3d3e8fdcc590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fact_check(claim:  str, top_k: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Kiểm tra tính xác thực của một tuyên bố dựa trên news sources.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    documents = hybrid_retrieve(\n",
    "        query=claim, \n",
    "        collection=collection, \n",
    "        alpha=0.3,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # Format sources\n",
    "    formatted_sources = \"\"\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        formatted_sources += f\"\"\"\n",
    "[Source {i}]\n",
    "Title: {doc['title']}\n",
    "Content: {doc['chunk'][:300]}...\n",
    "Date: {doc['pubDate']}\n",
    "URL: {doc['link']}\n",
    "\"\"\"\n",
    "    \n",
    "    PROMPT = f\"\"\"You are a fact-checker.  Analyze the following claim against the provided news sources. \n",
    "\n",
    "**Claim to verify**:  {claim}\n",
    "\n",
    "**News Sources**:\n",
    "{formatted_sources}\n",
    "\n",
    "**Instructions**:\n",
    "1. Carefully compare the claim with information in the sources\n",
    "2. Determine if the claim is:  true, false, partially_true, or unverifiable\n",
    "3. Provide specific evidence from the sources\n",
    "4. Be concise\n",
    "\n",
    "**Response Format** (use exactly this format):\n",
    "VERDICT: [true/false/partially_true/unverifiable]\n",
    "EVIDENCE: \n",
    "- [evidence point 1]\n",
    "- [evidence point 2]\n",
    "SOURCES:  [Source numbers used, e.g., 1, 3, 5]\n",
    "\"\"\"\n",
    "    \n",
    "    response = generate_with_single_input(PROMPT)\n",
    "    content = response['content']\n",
    "    \n",
    "    # Parse response manually instead of expecting JSON\n",
    "    result = {\n",
    "        \"claim\": claim,\n",
    "        \"verdict\": \"unverifiable\",\n",
    "        \"evidence\": [],\n",
    "        \"sources\":  [],\n",
    "        \"raw_response\": content\n",
    "    }\n",
    "    \n",
    "    # Try to extract verdict\n",
    "    lines = content.split('\\n')\n",
    "    for line in lines: \n",
    "        line_lower = line.lower().strip()\n",
    "        if line_lower.startswith('verdict:'):\n",
    "            verdict_text = line_lower. replace('verdict:', '').strip()\n",
    "            if 'true' in verdict_text and 'partially' not in verdict_text:\n",
    "                result['verdict'] = 'true'\n",
    "            elif 'false' in verdict_text: \n",
    "                result['verdict'] = 'false'\n",
    "            elif 'partially' in verdict_text: \n",
    "                result['verdict'] = 'partially_true'\n",
    "            else:\n",
    "                result['verdict'] = 'unverifiable'\n",
    "        elif line. strip().startswith('- '):\n",
    "            result['evidence'].append(line.strip()[2:])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def compare_events(event1: str, event2: str, top_k:  int = 5) -> str:\n",
    "    \"\"\"\n",
    "    So sánh hai sự kiện dựa trên news coverage.\n",
    "    \n",
    "    Args:\n",
    "        event1: Sự kiện thứ nhất\n",
    "        event2: Sự kiện thứ hai\n",
    "        top_k:  Số sources cho mỗi sự kiện\n",
    "        \n",
    "    Returns:\n",
    "        str: Phân tích so sánh\n",
    "    \"\"\"\n",
    "    # Retrieve documents cho cả hai events\n",
    "    docs1 = semantic_search_retrieve(query=event1, collection=collection, top_k=top_k)\n",
    "    docs2 = semantic_search_retrieve(query=event2, collection=collection, top_k=top_k)\n",
    "    \n",
    "    # Format sources\n",
    "    formatted_event1 = \"\\n\".join([\n",
    "        f\"- {doc['title']}: {doc['chunk'][: 200]}...\" for doc in docs1\n",
    "    ])\n",
    "    formatted_event2 = \"\\n\".join([\n",
    "        f\"- {doc['title']}: {doc['chunk'][:200]}...\" for doc in docs2\n",
    "    ])\n",
    "    \n",
    "    PROMPT = f\"\"\"Compare and contrast the following two events based on news coverage: \n",
    "\n",
    "**Event 1**: {event1}\n",
    "Sources: \n",
    "{formatted_event1}\n",
    "\n",
    "**Event 2**: {event2}\n",
    "Sources: \n",
    "{formatted_event2}\n",
    "\n",
    "**Provide**:\n",
    "1. Summary of each event\n",
    "2. Key similarities\n",
    "3. Key differences\n",
    "4. Relative media coverage/importance\n",
    "5. Any connections between the events\n",
    "\"\"\"\n",
    "    \n",
    "    response = generate_with_single_input(PROMPT, temperature=0.5)\n",
    "    return response['content']\n",
    "\n",
    "\n",
    "def summarize_topic(topic: str, time_range: str = \"2024\", top_k: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Tổng hợp tin tức về một chủ đề. \n",
    "    \"\"\"\n",
    "    # Retrieve documents\n",
    "    documents = hybrid_retrieve(\n",
    "        query=topic, \n",
    "        collection=collection, \n",
    "        alpha=0.6,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # Format sources with dates\n",
    "    formatted_sources = \"\"\n",
    "    for doc in sorted(documents, key=lambda x: str(x['pubDate'])):\n",
    "        formatted_sources += f\"\"\"\n",
    "Date: {doc['pubDate']}\n",
    "Title: {doc['title']}\n",
    "Summary: {doc['chunk'][:200]}... \n",
    "---\n",
    "\"\"\"\n",
    "    \n",
    "    PROMPT = f\"\"\"Create a comprehensive summary of news about \"{topic}\" in {time_range}. \n",
    "\n",
    "**Available News Articles**:\n",
    "{formatted_sources}\n",
    "\n",
    "**Provide your response in this format**: \n",
    "\n",
    "EXECUTIVE SUMMARY:\n",
    "[2-3 sentence overview]\n",
    "\n",
    "KEY POINTS:\n",
    "- [point 1]\n",
    "- [point 2]\n",
    "- [point 3]\n",
    "\n",
    "TIMELINE:\n",
    "- [Date]:  [Event description]\n",
    "- [Date]: [Event description]\n",
    "\n",
    "TRENDS:\n",
    "[Analysis of how the topic evolved]\n",
    "\"\"\"\n",
    "    \n",
    "    response = generate_with_single_input(PROMPT)\n",
    "    content = response['content']\n",
    "    \n",
    "    # Parse response manually\n",
    "    result = {\n",
    "        \"topic\": topic,\n",
    "        \"time_range\": time_range,\n",
    "        \"executive_summary\": \"\",\n",
    "        \"key_points\": [],\n",
    "        \"timeline\": [],\n",
    "        \"trends\":  \"\",\n",
    "        \"raw_response\": content\n",
    "    }\n",
    "    \n",
    "    # Extract sections\n",
    "    lines = content.split('\\n')\n",
    "    current_section = None\n",
    "    \n",
    "    for line in lines: \n",
    "        line_stripped = line.strip()\n",
    "        line_upper = line_stripped. upper()\n",
    "        \n",
    "        if 'EXECUTIVE SUMMARY' in line_upper:\n",
    "            current_section = 'summary'\n",
    "        elif 'KEY POINTS' in line_upper:\n",
    "            current_section = 'points'\n",
    "        elif 'TIMELINE' in line_upper: \n",
    "            current_section = 'timeline'\n",
    "        elif 'TRENDS' in line_upper:\n",
    "            current_section = 'trends'\n",
    "        elif line_stripped: \n",
    "            if current_section == 'summary' and not line_stripped.startswith('-'):\n",
    "                result['executive_summary'] += line_stripped + ' '\n",
    "            elif current_section == 'points' and line_stripped.startswith('-'):\n",
    "                result['key_points']. append(line_stripped[1:]. strip())\n",
    "            elif current_section == 'timeline' and line_stripped. startswith('-'):\n",
    "                result['timeline']. append(line_stripped[1:].strip())\n",
    "            elif current_section == 'trends' and not line_stripped.startswith('-'):\n",
    "                result['trends'] += line_stripped + ' '\n",
    "    \n",
    "    result['executive_summary'] = result['executive_summary']. strip()\n",
    "    result['trends'] = result['trends'].strip()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d6edd-bd51-4274-9fad-f6b372d0b453",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5 - Experimenting with Your RAG System\n",
    "\n",
    "Now it is time for you to experiment with the system! Run the next cell to load a widget that will input a query, a rerank property, and output five different LLM responses:\n",
    "\n",
    "1. With semantic search\n",
    "2. With semantic search and reranking\n",
    "3. With BM25 search\n",
    "4. With hybrid search\n",
    "5. Without RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b022b6-1a64-4725-9fe2-7d0b2afbe9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def enhanced_rag_demo():\n",
    "    \"\"\"\n",
    "    Demo tất cả tính năng của enhanced RAG system. \n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENHANCED RAG SYSTEM DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Demo 1: Auto-tuned Technical Query (không dùng rerank)\n",
    "    print(\"\\nDemo 1: Auto-tuned Technical Query\")\n",
    "    print(\"-\" * 40)\n",
    "    query1 = \"What was the GDP growth rate of the US in 2024?\"\n",
    "    try:\n",
    "        result1 = llm_call_advanced(query1, auto_tune=True, output_format=\"text\", use_rerank=False)\n",
    "        print(f\"Query: {query1}\")\n",
    "        print(f\"Detected Type: {result1['query_type']}\")\n",
    "        print(f\"Category:  {result1['category']}\")\n",
    "        print(f\"Params Used: {result1['params_used']}\")\n",
    "        print(f\"\\nResponse:\\n{result1['content'][: 500]}...\")\n",
    "    except Exception as e: \n",
    "        print(f\"Error in Demo 1: {e}\")\n",
    "    \n",
    "    # Demo 2: Creative query\n",
    "    print(\"\\n\\nDemo 2: Auto-tuned Creative Query\")\n",
    "    print(\"-\" * 40)\n",
    "    query2 = \"Analyze the political implications of US-Brazil relations in 2024\"\n",
    "    try:\n",
    "        result2 = llm_call_advanced(query2, auto_tune=True, output_format=\"text\", use_rerank=False)\n",
    "        print(f\"Query: {query2}\")\n",
    "        print(f\"Detected Type: {result2['query_type']}\")\n",
    "        print(f\"Category:  {result2['category']}\")\n",
    "        print(f\"\\nResponse:\\n{result2['content'][:500]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Demo 2: {e}\")\n",
    "    \n",
    "    # Demo 3: Với Reranking (sử dụng đúng function)\n",
    "    print(\"\\n\\nDemo 3: Query với Reranking\")\n",
    "    print(\"-\" * 40)\n",
    "    query3 = \"Tell me about Taylor Swift's concerts\"\n",
    "    try: \n",
    "        result3 = llm_call_advanced(\n",
    "            query3, \n",
    "            auto_tune=False,  # Tắt auto_tune để control params\n",
    "            use_rerank=True, \n",
    "            rerank_property='chunk',\n",
    "            top_k=5,\n",
    "            output_format=\"text\"\n",
    "        )\n",
    "        print(f\"Query: {query3}\")\n",
    "        print(f\"Using Reranking:  True\")\n",
    "        print(f\"\\nResponse:\\n{result3['content'][:500]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Demo 3: {e}\")\n",
    "    \n",
    "    # Demo 4: Fact checking\n",
    "    print(\"\\n\\nDemo 4: Fact Checking\")\n",
    "    print(\"-\" * 40)\n",
    "    claim = \"Taylor Swift broke the record at Wembley Stadium in 2024\"\n",
    "    try:\n",
    "        result4 = fact_check(claim)\n",
    "        print(f\"Claim: {claim}\")\n",
    "        print(f\"Verdict: {result4.get('verdict', 'N/A')}\")\n",
    "        evidence = result4.get('evidence', [])\n",
    "        if evidence:\n",
    "            print(f\"Evidence:  {evidence[: 2]}\")\n",
    "    except Exception as e: \n",
    "        print(f\"Error in Demo 4: {e}\")\n",
    "    \n",
    "    # Demo 5: Topic Summary\n",
    "    print(\"\\n\\nDemo 5: Topic Summary\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        result5 = summarize_topic(\"US Economy\", \"2024\", top_k=5)\n",
    "        print(f\"Topic:  US Economy in 2024\")\n",
    "        print(f\"Executive Summary: {result5. get('executive_summary', 'N/A')}\")\n",
    "        key_points = result5.get('key_points', [])\n",
    "        if key_points:\n",
    "            print(f\"Key Points:  {key_points[: 3]}\")\n",
    "    except Exception as e: \n",
    "        print(f\"Error in Demo 5: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DEMO COMPLETE\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1489d-d5f2-464d-a78a-0a6680bdc236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enhanced_rag_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ebb4fa-0111-4aa6-afbe-3b320430511a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_widget(llm_call, semantic_search_retrieve, bm25_retrieve, hybrid_retrieve, semantic_search_with_reranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2b655-44d0-4b72-814b-08b30e1ad724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
